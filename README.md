# Bachelor Thesis: GameStop - A Symptom of Systemic Weaknesses

Dieses Repository enth√§lt den gesamten Quellcode, die Daten und die Analyse f√ºr die Bachelorarbeit mit dem Titel: **"GameStop ‚Äì Ein Symptom systematischer Schw√§chen im Finanzmarkt: Eine empirische Analyse von Short Selling, Retail-Investoren und Social Media"**.

## üìù Projekt√ºbersicht

Dieses Projekt f√ºhrt eine empirische Analyse durch, um den Zusammenhang zwischen den Aktivit√§ten in Reddit-Communities (insbesondere r/wallstreetbets, r/gme, r/Superstonk) und den Marktbewegungen der GameStop-Aktie (GME) zu untersuchen. Der Fokus liegt darauf, quantitative Belege f√ºr den Einfluss von Social Media und koordinierten Retail-Investoren auf den Aktienkurs, das Handelsvolumen und die Volatilit√§t zu finden.

Der Workflow umfasst die Datenerhebung aus verschiedenen Quellen, die Bereinigung und Aufbereitung der Daten, die Sentiment-Analyse von Textdaten sowie eine umfassende statistische Auswertung mittels Korrelations- und Kausalit√§tsanalysen.

### Repository-Inhalte

*   `/Skripte:` Enth√§lt alle Python-Skripte (`.py`), die f√ºr den gesamten Prozess von der Datenerhebung bis zur finalen Analyse verwendet wurden.
*   `/Daten:` Enth√§lt die Rohdaten sowie die aufbereiteten CSV-Dateien, die von den Skripten generiert und verwendet werden. Das zentrale Ergebnis ist die `COMPREHENSIVE_ANALYSIS_DATA.csv`.
*   `/Ergebnisse:` Enth√§lt die finalen Ergebnisse der Analyse wie Grafiken (z.B. `heatmap_concurrent_correlation.png`) und den exportierten PDF-Bericht.
*   `Bachelorarbeit.pdf:` (Optional) Die finale schriftliche Ausarbeitung der Bachelorarbeit.

## üéì Informationen zur Abschlussarbeit

*   **Autor:** Moritz Waldmann
*   **Hochschule:** Hochschule M√ºnchen
*   **Studiengang:** Betriebswirtschaft
*   **Thema:** GameStop ‚Äì Ein Symptom systematischer Schw√§chen im Finanzmarkt: Eine empirische Analyse von Short Selling, Retail-Investoren und Social Media
*   **Betreuer:** [Name des Betreuers]
*   **Datum:** [Datum der Abgabe]

## üöÄ Erste Schritte

Um dieses Projekt lokal nachzubauen und die Analysen auszuf√ºhren, sind folgende Schritte erforderlich.

### Voraussetzungen

*   Python 3.8 oder h√∂her
*   Pip (Python Package Installer)
*   Git f√ºr das Klonen des Repositories

### Installation & Einrichtung

1.  **Repository klonen:**
    ```bash
    git clone https://github.com/Morizzz17/Bachelorarbeit_MoritzW.git
    cd Bachelorarbeit_MoritzW
    ```

2.  **Virtuelle Umgebung erstellen und aktivieren (empfohlen):**
    *Dadurch werden die Projekt-Abh√§ngigkeiten von anderen Python-Projekten isoliert.*
    ```bash
    # F√ºr Windows
    python -m venv venv
    .\venv\Scripts\activate

    # F√ºr macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Abh√§ngigkeiten installieren:**
    Es wird empfohlen, eine `requirements.txt`-Datei mit allen notwendigen Paketen zu erstellen. Die wichtigsten sind:
    *   `pandas`
    *   `numpy`
    *   `matplotlib`
    *   `seaborn`
    *   `statsmodels`
    *   `yfinance` (f√ºr Aktiendaten)
    *   `praw` (f√ºr Reddit-Daten)

    Installieren Sie diese mit:
    ```bash
    pip install pandas numpy matplotlib seaborn statsmodels yfinance praw
    ```

### Ausf√ºhrung

Die Skripte sind so konzipiert, dass sie in einer bestimmten Reihenfolge ausgef√ºhrt werden, um die Datenpipeline abzubilden.

1.  **Datenerhebung:**
    F√ºhren Sie die Scraper-Skripte aus, um die Rohdaten zu sammeln. *Hinweis: Dies kann aufgrund von API-Limits und der Datenmenge sehr lange dauern. Alternativ k√∂nnen die bereits erhobenen Rohdaten verwendet werden.*
    *   `wayback_auto_daily_downloader.py`
    *   `activity_scraper.py`
    *   `gme_stock_data.py`

2.  **Datenaufbereitung und -analyse:**
    Nach der Datenerhebung f√ºhren Sie die Verarbeitungs- und Analyse-Skripte aus.
    *   `data_cleaning.py` & `local_scraper_unified.py`: Bereinigen und konsolidieren die gesammelten Rohdaten.
    *   `sentiment_analysis.py`: F√ºhrt die Sentiment-Analyse durch und erstellt t√§gliche aggregierte Sentiment-Werte.

3.  **Statistische Auswertung:**
    Das Hauptskript `stat_relevance.py` f√ºhrt alle aufbereiteten Daten zusammen und f√ºhrt die finale statistische Analyse durch.
    ```bash
    python stat_relevance.py
    ```
    Dieses Skript generiert die `COMPREHENSIVE_ANALYSIS_DATA.csv` und gibt die statistischen Ergebnisse auf der Konsole aus.

4.  **Export der Ergebnisse:**
    Das Skript `stat_relevance_pdf_export.py` kann verwendet werden, um die wichtigsten Ergebnisse und Grafiken in einem PDF-Bericht zusammenzufassen.

## üõ†Ô∏è Code-Beschreibung

Die Skripte sind modular aufgebaut, um die verschiedenen Phasen des Projekts abzubilden.

### `activity_scraper.py`
*Dieses Skript sammelt Metriken zur Nutzeraktivit√§t wie die Anzahl der Abonnenten und t√§glichen Posts aus den relevanten Subreddits.*

**Details:** Nutzt wahrscheinlich die `PRAW`-Bibliothek, um auf die Reddit-API zuzugreifen. Es iteriert durch eine Liste von Ziel-Subreddits (r/wallstreetbets, r/gme, r/Superstonk), ruft Metadaten wie die aktuelle Abonnentenzahl und die neuesten Posts ab und aggregiert diese Daten auf einer t√§glichen Basis in einer CSV-Datei.

### `data_cleaning.py`
*Dieses Skript bereinigt die Rohdaten, indem es beispielsweise Duplikate entfernt, Formate vereinheitlicht und fehlende Werte behandelt.*

**Details:** L√§dt die rohen Post- und Kommentardaten, entfernt doppelte Eintr√§ge, konvertiert Zeitstempel in ein einheitliches Datumsformat und bereinigt Textdaten (z.B. Entfernung von URLs) als Vorbereitung f√ºr die Sentiment-Analyse.

### `gme_stock_data.py`
*Dieses Skript ruft die historischen Aktienkurs- und Volumendaten f√ºr GameStop (GME) von einer Finanzdatenquelle ab.*

**Details:** Verwendet die `yfinance`-Bibliothek, um f√ºr den Ticker "GME" und einen definierten Zeitraum die t√§glichen Daten (Open, High, Low, Close, Volume) herunterzuladen und in einer sauberen CSV-Datei (`gme_stock_data.csv`) zu speichern.

### `local_scraper_unified.py`
*Dies ist eine konsolidierte Version, die mehrere lokale Datenverarbeitungsschritte zu einem einheitlichen Skript zusammenfasst.*

**Details:** Dient als Wrapper oder kombiniertes Skript, das verschiedene lokal gespeicherte Datenquellen (z.B. heruntergeladene JSON- oder CSV-Dumps) durchsucht. Es filtert nach relevanten Keywords oder Zeitr√§umen und harmonisiert die Daten aus verschiedenen Quellen zu einem einheitlichen Format.

### `sentiment_analysis.py`
*Dieses Skript analysiert die gesammelten Reddit-Texte, um daraus t√§gliche Sentiment-Werte zu berechnen.*

**Details:** L√§dt die bereinigten Textdaten (Posts, Kommentare), initialisiert einen Sentiment-Analyzer (VADER) und iteriert durch jeden Text, um einen Sentiment-Score zu berechnen. Die Scores werden auf t√§glicher Basis aggregiert und als Durchschnittswerte (`Avg_Sentiment_VADER`, `Avg_Sentiment_Final`) in einer CSV-Datei gespeichert.

### `stat_relevance.py`
*Dieses Skript f√ºhrt die statistische Hauptanalyse durch, indem es die Reddit- und Aktiendaten zusammenf√ºhrt und Korrelationen sowie Kausalit√§tstests berechnet.*

**Details:** Dies ist das zentrale Analyse-Skript. Es l√§dt die Ergebnisse der vorherigen Skripte, f√ºhrt sie in einem umfassenden DataFrame zusammen und f√ºhrt ein Feature Engineering durch (Berechnung von `Price_Change`, `Log_Return` und verz√∂gerten Variablen). Es f√ºhrt ADF-Tests, Korrelationsanalysen und Granger-Kausalit√§tstests durch und gibt deren Ergebnisse aus. Der finale, aufbereitete DataFrame wird als `COMPREHENSIVE_ANALYSIS_DATA.csv` gespeichert.

### `stat_relevance_pdf_export.py`
*Dieses Skript exportiert die Ergebnisse und Visualisierungen der statistischen Analyse in ein formatiertes PDF-Dokument.*

**Details:** L√§dt die Ergebnisse und Grafiken aus dem Analyseprozess und verwendet Bibliotheken wie `matplotlib` und `FPDF`, um einen strukturierten Bericht mit den wichtigsten Tabellen und Grafiken zu erstellen.

### `wayback_auto_daily_downloader.py`
*Dieses Skript ist darauf ausgelegt, automatisiert t√§gliche Daten-Snapshots von einer Internet-Archivquelle wie der Wayback Machine herunterzuladen.*

**Details:** Dient dazu, historische Daten von Reddit zu sammeln, insbesondere f√ºr Zeitr√§ume, in denen die offizielle API keine vollst√§ndigen Daten mehr liefert.

## üìà Methodik

*   **Forschungsdesign:** Mixed-Methods-Ansatz, mit einem Schwerpunkt auf quantitativer Zeitreihenanalyse, erg√§nzt durch qualitative Experteninterviews.
*   **Datenerhebungsmethoden:** Prim√§rdatenerhebung von Reddit mittels API-Zugriff (`PRAW`, `Pushshift`) und Sekund√§rdatenerhebung von Finanzdaten (`yfinance`).
*   **Stichprobe / Auswahlverfahren:** Vollerhebung aller Posts/Kommentare in den Subreddits r/wallstreetbets, r/gme und r/Superstonk innerhalb des definierten Zeitraums sowie der t√§glichen GME-Kursdaten.
*   **Operationalisierung der Variablen:** Die Variablen wurden in Aktienmarkt- (Preis, Volumen, Log-Return), Reddit-Sentiment-, Reddit-Aktivit√§ts- und Reddit-Engagement-Metriken unterteilt und auf t√§glicher Basis aggregiert. Lagged-Variablen wurden zur Analyse zeitversetzter Effekte erstellt.
*   **Statistische Verfahren / Auswertungsmethoden:**
    *   Deskriptive Statistik
    *   Zeitreihen-Visualisierung
    *   Stationarit√§tstests (Augmented Dickey-Fuller)
    *   Korrelationsanalyse (Pearson r) f√ºr gleichzeitige und gelaggte Variablen
    *   Granger-Kausalit√§tstests zur Untersuchung der pr√§diktiven Kraft der Reddit-Metriken
