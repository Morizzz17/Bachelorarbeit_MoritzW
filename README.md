# Bachelor Thesis: GameStop & Social Media

Dieses Repository enth√§lt den gesamten Quellcode, die Daten und die Analyse f√ºr die Bachelorarbeit mit dem Titel: **"GameStop ‚Äì Ein Symptom systematischer Schw√§chen im Finanzmarkt: Eine empirische Analyse von Short Selling, Retail-Investoren und Social Media"**.

## üìù Projekt√ºbersicht

Dieses Projekt f√ºhrt eine empirische Analyse durch, um den Zusammenhang zwischen den Aktivit√§ten in Reddit-Communities (insbesondere r/wallstreetbets, r/gme, r/Superstonk) und den Marktbewegungen der GameStop-Aktie (GME) zu untersuchen. Der Fokus liegt darauf, quantitative Belege f√ºr den Einfluss von Social Media und koordinierten Retail-Investoren auf den Aktienkurs, das Handelsvolumen und die Volatilit√§t zu finden.

Der Workflow ist in mehrere Phasen unterteilt:
1.  **Datenerhebung:** Automatisches Herunterladen von historischen Reddit-Seiten und Abrufen von Aktivit√§ts-Metriken sowie Aktienkursdaten.
2.  **Datenverarbeitung:** Parsen der HTML-Dateien, Extraktion von Posts, Bereinigung der Textdaten und Durchf√ºhrung einer Sentiment-Analyse.
3.  **Analyse:** Zusammenf√ºhren aller Datenquellen und Durchf√ºhrung statistischer Tests (Korrelation, Granger-Kausalit√§t) zur √úberpr√ºfung der aufgestellten Hypothesen.

### Repository-Inhalte

*   `/Skripte:` Enth√§lt alle Python-Skripte (`.py`), die f√ºr den gesamten Prozess von der Datenerhebung bis zur finalen Analyse verwendet wurden.
*   `/Daten:` Enth√§lt die Rohdaten (z.B. heruntergeladene HTML-Dateien) sowie die aufbereiteten CSV-Dateien, die von den Skripten generiert und verwendet werden.
*   `/Ergebnisse:` Enth√§lt die finalen Ergebnisse der Analyse wie die umfassende CSV-Datei (`COMPREHENSIVE_ANALYSIS_DATA.csv`), Grafiken (z.B. `heatmap_concurrent_correlation.png`) und den generierten PDF-Bericht (`Statistische_Relevanzanalyse_Report.pdf`).

## üöÄ Erste Schritte

Um dieses Projekt lokal nachzubauen und die Analysen auszuf√ºhren, sind folgende Schritte erforderlich.

### Voraussetzungen

*   Python 3.8 oder h√∂her
*   Pip (Python Package Installer)
*   Git f√ºr das Klonen des Repositories

### Installation & Einrichtung

1.  **Repository klonen:**
    ```bash
    git clone https://github.com/Morizzz17/Bachelorarbeit_MoritzW.git
    cd Bachelorarbeit_MoritzW
    ```

2.  **Virtuelle Umgebung erstellen und aktivieren (empfohlen):**
    *Dadurch werden die Projekt-Abh√§ngigkeiten von anderen Python-Projekten isoliert.*
    ```bash
    # F√ºr Windows
    python -m venv venv
    .\venv\Scripts\activate

    # F√ºr macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Abh√§ngigkeiten installieren:**
    Die Skripte erfordern eine Reihe von Python-Paketen. Installieren Sie diese mit:
    ```bash
    pip install pandas numpy matplotlib seaborn statsmodels scikit-learn spacy beautifulsoup4 textblob requests tqdm fpdf2 vaderSentiment
    ```
    Laden Sie zus√§tzlich das spaCy-Sprachmodell herunter:
    ```bash
    python -m spacy download en_core_web_sm
    ```

### Ausf√ºhrung

Die Skripte sind so konzipiert, dass sie in einer bestimmten Reihenfolge ausgef√ºhrt werden, um die Datenpipeline abzubilden. Dar√ºberhinaus ist der Input und Output Ordner innerhalb der Skripte festzulegen. Hierf√ºr m√ºssen *OUTPUT_DIR* und gegebenenfalls *Input-Pfade* angepasst werden.

1.  **Datenerhebung (mehrere Quellen):**
    *   F√ºhren Sie `wayback_auto_daily_downloader.py` aus, um historische HTML-Snapshots der Reddit-Seiten zu sammeln.
    *   F√ºhren Sie `activity_scraper.py` aus, um t√§gliche Metriken zu Abonnenten, Posts und Kommentaren zu sammeln.
    *   F√ºhren Sie `gme_stock_data.py` aus, um die Aktienkursdaten aus der lokal gespeicherten HTML-Datei zu extrahieren.

2.  **Datenaufbereitung und -analyse:**
    *   F√ºhren Sie `local_scraper_unified.py` aus. Dieses Skript ist der zentrale Verarbeitungsschritt: Es parst die HTML-Dateien, bereinigt die Daten (mithilfe von `data_cleaning.py`), f√ºhrt die Sentiment-Analyse durch (mithilfe von `sentiment_analysis.py`) und erstellt die aggregierten CSV-Dateien `ALL_reddit_posts_extended.csv` und `ALL_reddit_sentiment_daily.csv`.

3.  **Statistische Auswertung:**
    *   F√ºhren Sie `stat_relevance.py` aus. Dieses Skript f√ºhrt alle aufbereiteten Daten zusammen, berechnet die finalen Kennzahlen und Lag-Variablen und f√ºhrt die statistischen Tests durch.
    *   Das Ergebnis ist die Konnektivit√§tsdatei `COMPREHENSIVE_ANALYSIS_DATA.csv` und eine Heatmap der Korrelationen.

4.  **Export der Ergebnisse:**
    *   F√ºhren Sie abschlie√üend `stat_relevance_pdf_export.py` aus, um die wichtigsten Ergebnisse in einem PDF-Bericht zusammenzufassen.

## üõ†Ô∏è Code-Beschreibung

Die Skripte sind modular aufgebaut, um die verschiedenen Phasen des Projekts abzubilden.

### `wayback_auto_daily_downloader.py`
*Dieses Skript ist darauf ausgelegt, automatisiert t√§gliche Daten-Snapshots von einer Internet-Archivquelle wie der Wayback Machine herunterzuladen.*

**Details:** Das Skript verwendet die Wayback CDX API, um f√ºr einen definierten Zeitraum t√§gliche HTML-Snapshots der angegebenen Subreddits zu finden und herunterzuladen. Es beinhaltet eine robuste Fehlerbehandlung und eine Retry-Logik, um fehlgeschlagene Downloads zu wiederholen und zu protokollieren.

### `activity_scraper.py`
*Dieses Skript sammelt Metriken zur Nutzeraktivit√§t wie die Anzahl der Abonnenten und t√§glichen Posts aus den relevanten Subreddits.*

**Details:** Anstatt Reddit direkt anzufragen, nutzt dieses Skript die Webseite `subredditstats.com`. Es kombiniert zwei Methoden: F√ºr Abonnentenzahlen verwendet es eine API-Anfrage, w√§hrend f√ºr t√§gliche Post- und Kommentarzahlen das HTML der Seite geparst und ein eingebettetes JSON-Objekt extrahiert wird. Die Daten werden aggregiert, verarbeitet (inkl. Forward-Fill-Logik) und in der Datei `subreddit_activity_stats.csv` gespeichert.

### `gme_stock_data.py`
*Dieses Skript ruft die historischen Aktienkurs- und Volumendaten f√ºr GameStop (GME) von einer Finanzdatenquelle ab.*

**Details:** Dieses Skript parst eine lokal gespeicherte HTML-Datei von Yahoo Finance. Es extrahiert die Datentabelle, bereinigt die numerischen Werte (z.B. Entfernung von Kommas) und speichert die relevanten Spalten (Datum, Preis, Volumen) in einer sauberen CSV-Datei. Es ist f√ºr eine einmalige Extraktion aus einer bestehenden Datei konzipiert.

### `local_scraper_unified.py`
*Dies ist eine konsolidierte Version, die mehrere lokale Datenverarbeitungsschritte zu einem einheitlichen Skript zusammenfasst.*

**Details:** Dies ist das Kernst√ºck der Text- und Post-Verarbeitung. Das Skript l√§dt die von `wayback_auto_daily_downloader.py` gesammelten HTML-Dateien, parst sie mit BeautifulSoup und extrahiert einzelne Posts. Es f√ºhrt eine Keyword-Filterung durch, extrahiert Metadaten wie Upvotes (mit komplexen Fallbacks f√ºr verschiedene HTML-Strukturen), f√ºhrt eine Sentiment-Analyse durch (mithilfe von `sentiment_analysis.py`), identifiziert Emojis, berechnet das Verh√§ltnis von Gro√übuchstaben, taggt Themen und extrahiert Entit√§ten mit spaCy. Das Skript f√ºhrt eine globale Deduplizierung √ºber alle Subreddits hinweg durch und speichert mehrere Ergebnisdateien: eine detaillierte Liste aller einzigartigen Posts (`ALL_reddit_posts_extended.csv`), t√§gliche aggregierte Metriken (`ALL_reddit_sentiment_daily.csv`) und eine Frequenzliste der am h√§ufigsten verwendeten W√∂rter.

### `sentiment_analysis.py`
*Dieses Skript analysiert die gesammelten Reddit-Texte, um daraus t√§gliche Sentiment-Werte zu berechnen.*

**Details:** Dieses Hilfsskript initialisiert den VADER Sentiment-Analyzer und erweitert dessen Standard-Lexikon um ein benutzerdefiniertes W√∂rterbuch (`wsb_lexicon`). Dieses Lexikon enth√§lt spezifische Begriffe und Emojis aus der Finanz- und Reddit-Kultur (z.B. "tendies", "diamond hands", "üöÄ") mit angepassten Sentiment-Werten, um eine pr√§zisere Analyse im Kontext der GME-Diskussionen zu erm√∂glichen. Die Hauptfunktion `analyze_sentiment` wird von anderen Skripten importiert.

### `data_cleaning.py`
*Dieses Skript bereinigt die Rohdaten, indem es beispielsweise Duplikate entfernt, Formate vereinheitlicht und fehlende Werte behandelt.*

**Details:** Ein einfaches Hilfsskript, das eine `clean_text`-Funktion bereitstellt. Diese Funktion entfernt URLs und Sonderzeichen aus einem Text und konvertiert ihn in Kleinbuchstaben, um die Daten f√ºr die NLP-Verarbeitung vorzubereiten.

### `stat_relevance.py`
*Dieses Skript f√ºhrt die statistische Hauptanalyse durch, indem es die Reddit- und Aktiendaten zusammenf√ºhrt und Korrelationen sowie Kausalit√§tstests berechnet.*

**Details:** Dieses Skript ist der finale Schritt der quantitativen Analyse. Es l√§dt die aggregierten Tagesdaten aus den vorherigen Schritten (`ALL_reddit_sentiment_daily.csv`, `subreddit_activity_stats_wide.csv`, `gme_stock_data.csv`), f√ºhrt sie zu einem umfassenden Zeitreihen-DataFrame zusammen und bereitet diesen auf: Es f√ºllt fehlende Werte, berechnet finanzielle Kennzahlen wie logarithmische Renditen und erstellt verz√∂gerte (Lagged) Variablen. Anschlie√üend f√ºhrt es die statistischen Tests durch: Augmented Dickey-Fuller (ADF) zur Pr√ºfung der Stationarit√§t, Pearson-Korrelationsanalysen (gleichzeitig und zeitversetzt) und Granger-Kausalit√§tstests. Die Ergebnisse werden auf der Konsole ausgegeben und die finale Datentabelle wird als `COMPREHENSIVE_ANALYSIS_DATA.csv` f√ºr weitere Analysen und Visualisierungen gespeichert.

### `stat_relevance_pdf_export.py`
*Dieses Skript exportiert die Ergebnisse und Visualisierungen der statistischen Analyse in ein formatiertes PDF-Dokument.*

**Details:** Dieses Skript nimmt die numerischen Ergebnisse und die generierte Heatmap-Grafik aus `stat_relevance.py` und verwendet die `fpdf2`-Bibliothek, um einen zusammenfassenden, mehrseitigen Bericht im PDF-Format zu erstellen. Es beinhaltet formatierte Tabellen f√ºr deskriptive Statistiken und Korrelationsmatrizen sowie die eingebettete Heatmap-Grafik, um eine √ºbersichtliche Pr√§sentation der Ergebnisse zu gew√§hrleisten.

## üìà Methodik

*   **Forschungsdesign:** Mixed-Methods-Ansatz, mit einem Schwerpunkt auf quantitativer Zeitreihenanalyse, erg√§nzt durch qualitative Experteninterviews.
*   **Datenerhebungsmethoden:** Prim√§rdatenerhebung von Reddit mittels Scraping von `subredditstats.com` und der Wayback Machine; Sekund√§rdatenerhebung von Finanzdaten durch Parsen einer lokalen HTML-Datei von Yahoo Finance.
*   **Stichprobe / Auswahlverfahren:** Vollerhebung aller Posts/Kommentare in den Subreddits r/wallstreetbets, r/gme und r/Superstonk innerhalb des definierten Zeitraums (gefiltert nach Keywords) sowie der t√§glichen GME-Kursdaten.
*   **Operationalisierung der Variablen:** Die Variablen wurden in Aktienmarkt- (Preis, Volumen, Log-Return), Reddit-Sentiment-, Reddit-Aktivit√§ts- und Reddit-Engagement-Metriken unterteilt und auf t√§glicher Basis aggregiert. Lagged-Variablen wurden zur Analyse zeitversetzter Effekte erstellt.
*   **Statistische Verfahren / Auswertungsmethoden:**
    *   Deskriptive Statistik
    *   Zeitreihen-Visualisierung
    *   Stationarit√§tstests (Augmented Dickey-Fuller)
    *   Korrelationsanalyse (Pearson r) f√ºr gleichzeitige und gelaggte Variablen
    *   Granger-Kausalit√§tstests zur Untersuchung der pr√§diktiven Kraft der Reddit-Metriken
